{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441725b6-c01f-4e5d-b539-ee1041348098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    23481\n",
      "1    21417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "fake = pd.read_csv('Fake.csv')\n",
    "true = pd.read_csv('True.csv')\n",
    "\n",
    "# Add labels\n",
    "fake['label'] = 0\n",
    "true['label'] = 1\n",
    "\n",
    "# Merge datasets\n",
    "combined = pd.concat([fake, true], axis=0)\n",
    "\n",
    "# Handle missing values\n",
    "combined.dropna(inplace=True)\n",
    "\n",
    "# Combine title and text\n",
    "combined['content'] = combined['title'] + ' ' + combined['text']\n",
    "\n",
    "# Analyze class distribution\n",
    "print(combined['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2d08a0-67b2-4169-ba10-1ced44d7a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Text preprocessing\n",
    "X = combined['content']\n",
    "y = combined['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization and Feature Selection\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
    "    ('select', SelectKBest(chi2, k=5000))\n",
    "])\n",
    "\n",
    "X_train_transformed = pipeline.fit_transform(X_train, y_train)\n",
    "X_test_transformed = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b091e4-04b0-4bc6-a2ff-5271449256aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9879670677644079, 'recall': 0.9888020283118529, 'f1-score': 0.9883843717001056, 'support': 4733.0}, '1': {'precision': 0.9875088380862598, 'recall': 0.9865787614786908, 'f1-score': 0.9870435806831567, 'support': 4247.0}, 'accuracy': 0.987750556792873, 'macro avg': {'precision': 0.9877379529253338, 'recall': 0.9876903948952719, 'f1-score': 0.9877139761916311, 'support': 8980.0}, 'weighted avg': {'precision': 0.987750352681658, 'recall': 0.987750556792873, 'f1-score': 0.9877502581757199, 'support': 8980.0}} 0.987750556792873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "predictions = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    preds = model.predict_proba(X_test_transformed)[:, 1]  # Probability of class 1\n",
    "    predictions.append(preds)\n",
    "\n",
    "# Ensemble (average predictions)\n",
    "ensemble_preds = sum(predictions) / len(predictions)\n",
    "ensemble_preds_class = (ensemble_preds > 0.5).astype(int)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_report = classification_report(y_test, ensemble_preds_class, output_dict=True)\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_preds_class)\n",
    "print(ensemble_report, ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c49883fe-24a9-4b1e-8a18-b7d1f3760301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ensemble model\n",
    "ensemble_results = pd.DataFrame({\n",
    "    'True Label': y_test,\n",
    "    'Ensemble Prediction': ensemble_preds_class,\n",
    "    'Confidence': ensemble_preds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37855818-73e5-491e-854a-d44c1a15cde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Create Ensemble Model Class\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models):\n",
    "        self.models = models  # Dictionary of models\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for model in self.models.values():\n",
    "            model.fit(X, y)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predictions = np.zeros((X.shape[0], 2))  # Assuming binary classification\n",
    "        for model in self.models.values():\n",
    "            predictions += model.predict_proba(X)\n",
    "        return predictions / len(self.models)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] > 0.5).astype(int)\n",
    "    def print_models(self):\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"{name}: {model}\")\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model = EnsembleModel(models)\n",
    "ensemble_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Directory to save model\n",
    "output_dir = 'saved_models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the ensemble model\n",
    "joblib.dump(ensemble_model, f'{output_dir}/ensemble_model.pkl')\n",
    "\n",
    "print(\"Ensemble model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae4d948d-ff99-490d-ad97-09354f931419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline and ensemble model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 1. Train the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
    "    ('select', SelectKBest(chi2, k=5000))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data using the pipeline\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "# 2. Train individual models (already in your script)\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Train models and save the ensemble\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Create and save the ensemble model\n",
    "ensemble_model = EnsembleModel(models)\n",
    "ensemble_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Save the ensemble model\n",
    "joblib.dump(ensemble_model, 'saved_models/ensemble_model.pkl')\n",
    "\n",
    "# 3. Save the pipeline\n",
    "joblib.dump(pipeline, 'saved_models/pipeline.pkl')\n",
    "print(\"Pipeline and ensemble model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f819bb94-06ea-4bab-bdf9-5b40b6345bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b64a861c-ef37-4e18-8632-0118cb483833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: shap in c:\\users\\thema\\appdata\\roaming\\python\\python312\\site-packages (0.46.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (4.66.5)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (24.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\thema\\appdata\\roaming\\python\\python312\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8162c22a-d9b9-4f77-a356-fdf77154ff41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 7439!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mPermutationExplainer(model\u001b[38;5;241m.\u001b[39mpredict, background)\n\u001b[1;32m---> 18\u001b[0m     shap_values \u001b[38;5;241m=\u001b[39m explainer(test_sample)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Handle dimensional mismatch\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     shap_values_model \u001b[38;5;241m=\u001b[39m shap_values\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mif\u001b[39;00m shap_values\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m shap_values\u001b[38;5;241m.\u001b[39mvalues[:, :, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\shap\\explainers\\_permutation.py:77\u001b[0m, in \u001b[0;36mPermutationExplainer.__call__\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     75\u001b[0m              outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;241m*\u001b[39margs, max_evals\u001b[38;5;241m=\u001b[39mmax_evals, main_effects\u001b[38;5;241m=\u001b[39mmain_effects, error_bounds\u001b[38;5;241m=\u001b[39merror_bounds, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     79\u001b[0m         outputs\u001b[38;5;241m=\u001b[39moutputs, silent\u001b[38;5;241m=\u001b[39msilent\n\u001b[0;32m     80\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\shap\\explainers\\_explainer.py:266\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[1;32m--> 266\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain_row(\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;241m*\u001b[39mrow_args, max_evals\u001b[38;5;241m=\u001b[39mmax_evals, main_effects\u001b[38;5;241m=\u001b[39mmain_effects, error_bounds\u001b[38;5;241m=\u001b[39merror_bounds,\n\u001b[0;32m    268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size, outputs\u001b[38;5;241m=\u001b[39moutputs, silent\u001b[38;5;241m=\u001b[39msilent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    269\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    271\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\shap\\explainers\\_permutation.py:157\u001b[0m, in \u001b[0;36mPermutationExplainer.explain_row\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[0;32m    154\u001b[0m     history_pos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m npermutations \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_evals=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_evals\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m2\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(inds)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    159\u001b[0m expected_value \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# compute the main effects if we need to\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 7439!"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to dense if sparse\n",
    "background = X_train_transformed.toarray()  # Use a small sample as background\n",
    "test_sample = X_test_transformed.toarray()   # Take a subset of test data\n",
    "\n",
    "shap_values_ensemble = np.zeros(test_sample.shape)\n",
    "\n",
    "for name, model in models.items():\n",
    "    if isinstance(model, MultinomialNB):\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "        shap_values = explainer.shap_values(test_sample)\n",
    "        shap_values_model = np.array(shap_values[1])  # Class 1 SHAP values\n",
    "    else:\n",
    "        explainer = shap.PermutationExplainer(model.predict, background)\n",
    "        shap_values = explainer(test_sample)\n",
    "\n",
    "        # Handle dimensional mismatch\n",
    "        shap_values_model = shap_values.values if shap_values.values.ndim == 2 else shap_values.values[:, :, 1]\n",
    "    \n",
    "    # Aggregate SHAP values for ensemble\n",
    "    shap_values_ensemble += shap_values_model\n",
    "\n",
    "shap_values_ensemble /= len(models)\n",
    "\n",
    "plt.title(\"SHAP Summary Plot - Ensemble Model\")\n",
    "shap.summary_plot(shap_values_ensemble, test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d0692-f7ed-4565-8f50-f2d442852ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "ensemble_results.to_csv(f'{output_dir}/ensemble_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ebeae-3ad3-415d-a8b4-7adc7eeabc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from web3 import Web3\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "w3 = Web3(Web3.HTTPProvider('https://rinkeby.infura.io/v3/YOUR_INFURA_PROJECT_ID'))\n",
    "contract_address = '0xYourContractAddress'\n",
    "contract_abi = [...]  # ABI of the smart contract\n",
    "contract = w3.eth.contract(address=contract_address, abi=contract_abi)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json['text']\n",
    "    transformed_text = pipeline.transform([data])\n",
    "    prediction = models['Logistic Regression'].predict(transformed_text)\n",
    "    tx_hash = contract.functions.logResult(int(prediction[0])).transact({'from': w3.eth.accounts[0]})\n",
    "    receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\n",
    "    return jsonify({'prediction': int(prediction[0]), 'transaction_hash': receipt.transactionHash.hex()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f78f5-a389-4d8c-9216-55f1cc2bb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title('TruthGuard Dashboard')\n",
    "user_input = st.text_area(\"Enter news content to verify:\")\n",
    "if user_input:\n",
    "    transformed_text = pipeline.transform([user_input])\n",
    "    prediction = models['Logistic Regression'].predict(transformed_text)\n",
    "    st.write('Prediction: ', 'Fake News' if prediction == 0 else 'True News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29504e7-cb67-43c9-a914-17c2796b05c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fbf96-bfb6-4a4d-bb51-ba21e0636670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2aa068-349d-464f-8a20-c6ec8d13df89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "612bb4ea-61c7-48a7-ad02-bcf02a2ff819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and pipeline loaded successfully.\n",
      "Loaded Ensemble Model Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4733\n",
      "           1       0.99      0.99      0.99      4247\n",
      "\n",
      "    accuracy                           0.99      8980\n",
      "   macro avg       0.99      0.99      0.99      8980\n",
      "weighted avg       0.99      0.99      0.99      8980\n",
      "\n",
      "Accuracy: 0.9879\n"
     ]
    }
   ],
   "source": [
    "#Code to load models:\n",
    "import joblib\n",
    "\n",
    "# Load models\n",
    "loaded_models = {}\n",
    "for name in models.keys():\n",
    "    model_path = f'{output_dir}/{name.replace(\" \", \"_\")}_model.pkl'\n",
    "    loaded_models[name] = joblib.load(model_path)\n",
    "\n",
    "# Load pipeline\n",
    "loaded_pipeline = joblib.load(f'{output_dir}/text_pipeline.pkl')\n",
    "\n",
    "print(\"Models and pipeline loaded successfully.\")\n",
    "\n",
    "# Load ensemble model\n",
    "loaded_ensemble = joblib.load(f'{output_dir}/ensemble_model.pkl')\n",
    "\n",
    "# Predict on new data\n",
    "y_pred_ensemble = loaded_ensemble.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the loaded ensemble model\n",
    "print(\"Loaded Ensemble Model Results:\")\n",
    "print(classification_report(y_test, y_pred_ensemble))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_ensemble):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5c5f9-abba-4e38-a067-b6279b6047c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ensemble model\n",
    "loaded_ensemble = joblib.load(f'{output_dir}/ensemble_model.pkl')\n",
    "\n",
    "# Predict on new data\n",
    "y_pred_ensemble = loaded_ensemble.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the loaded ensemble model\n",
    "print(\"Loaded Ensemble Model Results:\")\n",
    "print(classification_report(y_test, y_pred_ensemble))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_ensemble):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f7661-a06a-49e8-8d93-1bb64490e35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
